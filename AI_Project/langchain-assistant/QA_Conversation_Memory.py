from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("your_file.txt", encoding="utf-8")
docs = loader.load()

# 2. åˆ‡åˆ†æ–‡æ¡£
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

# 3. è½¬å‘é‡
embedding = OllamaEmbeddings(model="llama3")
db = Chroma.from_documents(chunks, embedding)

# 4. åˆå§‹åŒ–æ¨¡å‹
llm = Ollama(model="llama3")

# 5. åˆå§‹åŒ–è®°å¿†
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# 6. æ„å»ºå¤šè½®é—®ç­”é“¾
qa = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=db.as_retriever(),
    memory=memory,
    return_source_documents=True,
    verbose=True,
)

# 7. äº¤äº’
print("ğŸ“š å¤šè½®æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œè¾“å…¥ exit é€€å‡º")
while True:
    query = input("\nä½ ï¼š")
    if query.lower() in ["exit", "quit"]:
        break
    result = qa({"question": query})
    print("\nğŸ¤– AIï¼š", result["answer"])
